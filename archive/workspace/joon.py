### cnn.py의 output을 classifier에 넣어 final prediction을 구합니다

### FC 쓸지 LSTM 쓸지 정하기
### Batch 적용 방법 생각해보기

### Output : Prediction of 0, 1, or 2 for each window (second)
### Input : Sequence for each window
### Assumed Input (One video) : 4 (window) x 1024 (flattened output of CNN for each frame)
### Assumed Output : 4 (window) x 1 (final prediction value of 0, 1, or 2)

# import torch
# import torchvision
# import torchvision.transforms as transforms
# import matplotlib.pyplot as plt
# import numpy as np
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.optim as optim

# ## [Training a Classifier — PyTorch Tutorials 2.0.0+cu117 documentation](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)
# ## Create dummy data using CIFAR10 dataset
# # Define transform and batch size
# transform = transforms.Compose(
#     [transforms.ToTensor(),
#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
# batch_size = 4

# trainset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True, transform = transform)
# trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle = True, num_workers = 2)

# testset = torchvision.datasets.CIFAR10(root = './data', train = False, download = True, transform = transform)
# testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle = False, num_workers = 2)

# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')


# # Define a CNN
# class Net(nn.Module):
#     def __init__(self):
#         super().__init__()
#         self.conv1 = nn.Conv2d(3, 6, 5)
#         self.pool = nn.MaxPool2d(2, 2)
#         self.conv2 = nn.Conv2d(6, 16, 5)
#         self.fc1 = nn.Linear(16 * 5 * 5, 120)
#         self.fc2 = nn.Linear(120, 84)
#         self.fc3 = nn.Linear(84, 10)
#         self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden, dropout=0.3)
#         self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))
#         self.b = nn.Parameter(torch.randn([n_class]).type(dtype))
#         self.Softmax = nn.Softmax(dim=1)

#     def forward(self, x):
#         x = self.pool(F.relu(self.conv1(x)))
#         x = self.pool(F.relu(self.conv2(x)))
#         x = torch.flatten(x, 1) # flatten all dimensions except batch
#         x = F.relu(self.fc1(x))
#         x = F.relu(self.fc2(x))
#         x = self.fc3(x)
#         return x
# net = Net()

#   def forward(self, hidden_and_cell, X):
#     X = X.transpose(0, 1)
#     outputs, hidden = self.lstm(X, hidden_and_cell)
#     outputs = outputs[-1]  # 최종 예측 Hidden Layer
#     model = torch.mm(outputs, self.W) + self.b  # 최종 예측 최종 출력 층
#     return model



# # Define a Loss function and optimizer
# criterion = nn.CrossEntropyLoss()
# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

# # Train the network
# for epoch in range(2):  # loop over the dataset multiple times

#     running_loss = 0.0
#     for i, data in enumerate(trainloader, 0):
#         # get the inputs; data is a list of [inputs, labels]
#         inputs, labels = data

#         # zero the parameter gradients
#         optimizer.zero_grad()

#         # forward + backward + optimize
#         outputs = net(inputs)
#         loss = criterion(outputs, labels)
#         loss.backward()
#         optimizer.step()

#         # print statistics
#         running_loss += loss.item()
#         if i % 2000 == 1999:    # print every 2000 mini-batches
#             print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')
#             running_loss = 0.0
# print('Finished Training')
# PATH = './cifar_net.pth'
# torch.save(net.state_dict(), PATH)


"""Dashcam_Conv3D.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fcoF8d7zcH2shvewT6Cvj64_oFywAPoP

# Feature Extraction from Dashcam video (Using Conv3D)

video info
- length : 04.000 sec
- fps : 25
- video total frames : 100
- resolution : 1280 x 720

모델링
- window size = 10 frames로 데이터셋을 분할한다.
- video resolution = 128 x 128로 고정한다.
- 이 후, Conv3D를 통해 각 window에 대한 feature extraction을 수행한다.

### Video 불러오기
"""

import random
import pandas as pd
import numpy as np
import os
import cv2 #영상처리를 위해 opencv를 사용한다

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import albumentations as A #데이터 augmentation
from albumentations.pytorch.transforms import ToTensorV2
import torchvision.models as models

from tqdm.auto import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score

import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings(action='ignore')

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

CFG = {
    'VIDEO_LENGTH':25 * 4, # 25프레임 * 4초
    'IMG_SIZE':128,
    'WINDOW_SIZE' : 10
}

# Commented out IPython magic to ensure Python compatibility.
# from google.colab import drive

# drive.mount('/content/drive')

# %cd drive/MyDrive/SKT Fellowship/사이드 프로젝트_알잘딱깔블/dummy_data_진규

def seperate_to_windows10(single_video):
    windows = []
    single_window = []
    for i in range(CFG['VIDEO_LENGTH']):
        single_window.append(single_video[:,i,:,:])
        if (i+1) % 10 == 0:
            windows.append(single_window)
            single_window = []
    
    return np.array(windows)

def get_video(path):
    frames = []
    cap = cv2.VideoCapture(path) #영상 파일(객체) 가져오기, opencv로 영상을 읽는 방법
    for _ in range(CFG['VIDEO_LENGTH']): #100장을 가져오겠다는 의미 - 미리 정의한 하이퍼파라미터 참고
        _, img = cap.read() #영상 파일에서 프레임 읽기
        img = cv2.resize(img, (CFG['IMG_SIZE'], CFG['IMG_SIZE']))
        img = img / 255.
        frames.append(img)
    
    frames_array = np.array(frames).transpose(3,0,1,2)
    frames_window = seperate_to_windows10(frames_array)

    return torch.FloatTensor(frames_window)

data = get_video('./data/000001.mp4')

print('채널, 프레임, 해상도, 해상도 : ', data.shape)

for i in range(CFG['WINDOW_SIZE']):
    img = data[0,i,:,:,:].squeeze()
    img_print = torch.permute(img,(1,2,0))
    plt.imshow(img_print)
    plt.show()

window_1 = data[0,:,:,:,:].squeeze()

print('window_1 shape : ', window_1.shape)
print('첫 번째 window는 ',window_1.shape[0],'장,','이미지 resolution은 ', window_1.shape[2],window_1.shape[3],window_1.shape[1])

"""### C3D 모델링

C3D를 통해 window 하나에 대한 feature extraction을 진행하겠다

- input (pytorch video) : [batch_size, channel, number_of_frame, height, width]
- output : 1차원 ([1,?])으로 반환될 예정

input이 [C,D,H,W] 순인 이유 : temporal 정보를 반영하기 위함이다. D,H,W로 블록을 구성하고 그 블록들이 C개 있다고 생각하자 (기존 Kernel 수가 C가 되고, C가 D(number of frames)가 된다
"""

class BaseModel_onlyfeature(nn.Module):
    def __init__(self, num_classes = 2):
        super(BaseModel_onlyfeature, self).__init__() #상속된 Module의 __init__ 변수도 사용하겠다
        self.feature_extract = nn.Sequential(
            nn.Conv3d(3,8,(3,3,3)),
            nn.ReLU(),
            nn.BatchNorm3d(8), #batchnorm의 input channel을 변수로 준다
            nn.MaxPool3d(2),#height,width를 (2,2)로 maxpool 해준다
            nn.Conv3d(8,32,(2,2,2)),
            nn.ReLU(),
            nn.BatchNorm3d(32),
            nn.MaxPool3d(2)
        )
        self.classifier = nn.Sequential(
            nn.Linear(1024, 120),
            nn.ReLU(),
            nn.Linear(120, 84),
            nn.ReLU(),
            nn.Linear(84,3)
        )
        
        # nn.LSTM(input_size=10, hidden_size=1024)
        # nn.Linear(1024,num_classes)
    def forward(self, x):
        batch_size = x.size(0)
        x = self.feature_extract(x)
        x = x.view(batch_size, -1)
        x = self.classifier(x)

        return x
    

# class Net(nn.Module):
#     def __init__(self):
#         super().__init__()
#         self.conv1 = nn.Conv2d(3, 6, 5)
#         self.pool = nn.MaxPool2d(2, 2)
#         self.conv2 = nn.Conv2d(6, 16, 5)
#         self.fc1 = nn.Linear(16 * 5 * 5, 120)
#         self.fc2 = nn.Linear(120, 84)
#         self.fc3 = nn.Linear(84, 10)

#     def forward(self, x):
#         x = self.pool(F.relu(self.conv1(x)))
#         x = self.pool(F.relu(self.conv2(x)))
#         x = torch.flatten(x, 1) # flatten all dimensions except batch
#         x = F.relu(self.fc1(x))
#         x = F.relu(self.fc2(x))
#         x = self.fc3(x)
#         return x
# net = Net()

model_onlyfeature = BaseModel_onlyfeature()

model_onlyfeature.to(device)
model_onlyfeature.train()

window_1 = window_1.view([-1,window_1.shape[0],window_1.shape[1],window_1.shape[2],window_1.shape[3]]).permute(0,2,1,3,4) 
window_1 = window_1.to(device)
#pytorch video input shape : [batch_size, channel, number_of_frame, height, width]
window_1.shape

output = model_onlyfeature(window_1)

output.shape

print(model_onlyfeature)

import torchsummary

torchsummary.summary(model_onlyfeature,(3,10,128,128))

import pytorch_model_summary

print(pytorch_model_summary.summary(model_onlyfeature,torch.zeros(1,3,10,128,128),show_input = True))







print('Hello World')